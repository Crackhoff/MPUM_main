{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook we use tensorflow CNN model to find waldo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:27:30.549723Z",
     "start_time": "2024-06-09T13:27:30.545629Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from img_gen import create_sample\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:27:30.560189Z",
     "start_time": "2024-06-09T13:27:30.556608Z"
    }
   },
   "outputs": [],
   "source": [
    "#constants\n",
    "IMG_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:27:30.586282Z",
     "start_time": "2024-06-09T13:27:30.582887Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating waldo images\n",
    "\n",
    "# create_sample(IMG_SIZE,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:27:30.997837Z",
     "start_time": "2024-06-09T13:27:30.588782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9354 files belonging to 2 classes.\n",
      "Using 7484 files for training.\n",
      "Using 1870 files for validation.\n",
      "['notwaldo', 'waldo']\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    '../data/selfmade',\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32)\n",
    "\n",
    "print(test_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:32:50.770251Z",
     "start_time": "2024-06-09T13:32:50.615557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5376 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "src1_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    '../data/src1/64',\n",
    "    seed=123,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:27:31.367751Z",
     "start_time": "2024-06-09T13:27:31.365797Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 23:13:08.508572: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [5376]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2024-06-11 23:13:08.508784: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [5376]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2024-06-11 23:13:08.513897: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5376, 3, 64, 64) (5376, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "# change src1_ds to arrays of x and y, y should be [[1.],[0.]] for waldo and [[0.],[1.]] for not waldo\n",
    "\n",
    "\n",
    "src1_x = np.zeros((5376,3,64,64))\n",
    "src1_y = np.zeros((5376,2,1))\n",
    "\n",
    "for i, (images, labels) in enumerate(src1_ds):\n",
    "    images = images.numpy().transpose(0,3,1,2)\n",
    "    for j in range(32):\n",
    "        # replace dimensions in images to 3x64x64\n",
    "        src1_x[i*32+j] = images[j]\n",
    "        src1_y[i*32+j] = [labels[j]]\n",
    "\n",
    "print(src1_x.shape, src1_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:27:31.373189Z",
     "start_time": "2024-06-09T13:27:31.368949Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Lambda, Resizing\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from cnn import NeuralNetwork\n",
    "from layers import ConvLayer, MaxPoolLayer, FlattenLayer, DenseLayer\n",
    "\n",
    "def get_conv(input_shape=(IMG_SIZE, IMG_SIZE, 3), filename=None):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_my_cnn(input_shape=(3,IMG_SIZE,IMG_SIZE)):\n",
    "    model = NeuralNetwork()\n",
    "    model.add(ConvLayer(32, 3, input_shape=input_shape, activation='relu'))\n",
    "    model.add(MaxPoolLayer((32, 62,62), (2, 2)))\n",
    "    model.add(ConvLayer(64, 3,(32, 31, 31), activation='relu'))\n",
    "    model.add(MaxPoolLayer((64, 29, 29), (2, 2)))\n",
    "    model.add(ConvLayer(64, 3, (64, 14, 14), activation='relu'))\n",
    "    model.add(MaxPoolLayer((64, 12, 12), (2, 2)))\n",
    "    model.add(FlattenLayer((64, 6, 6)))\n",
    "    model.add(DenseLayer((2304,), 1024, activation='relu'))\n",
    "    model.add(DenseLayer((1024,), 2, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:27:32.105340Z",
     "start_time": "2024-06-09T13:27:31.374302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2360320   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,418,690\n",
      "Trainable params: 2,418,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "model = get_conv()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model2 = get_my_cnn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:27:32.174309Z",
     "start_time": "2024-06-09T13:27:32.106624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 1104 elements\n",
      "Class 1: 766 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 23:13:09.467750: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [1870]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2024-06-11 23:13:09.467918: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [1870]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counter = Counter()\n",
    "\n",
    "# Iterate through the dataset\n",
    "for images, labels in test_ds:\n",
    "    # Convert the labels to numpy array if they are in tensor format\n",
    "    labels_np = labels.numpy()\n",
    "    \n",
    "    # Update the counter with the labels in the current batch\n",
    "    label_counter.update(labels_np)\n",
    "\n",
    "# Print the number of elements for each class\n",
    "for label, count in label_counter.items():\n",
    "    print(f\"Class {label}: {count} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:28:46.833884Z",
     "start_time": "2024-06-09T13:27:32.175827Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [39:09<00:00, 13.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Error 0.06501116071428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [1:02:17<00:00, 22.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Error 0.06501116071428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 62/168 [14:44<25:12, 14.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m               loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# both = yesset.concatenate(train_ds)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc1_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc1_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrad_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_ds, validation_data\u001b[38;5;241m=\u001b[39mtest_ds, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# history2 = model2.fit(train_ds, validation_data=test_ds, epochs=10)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/studia/mpum/MPUM_main/code/cnn.py:39\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, X, y, learning_rate, epochs, batch_size, optimizer, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     37\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 39\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(y_batch, y_pred)\n\u001b[1;32m     42\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_d(y_batch, y_pred)\n",
      "File \u001b[0;32m~/Desktop/studia/mpum/MPUM_main/code/cnn.py:64\u001b[0m, in \u001b[0;36mNeuralNetwork._forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# print(l, X.shape)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/Desktop/studia/mpum/MPUM_main/code/layers.py:195\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(channel):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, x \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m--> 195\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, y \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    196\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput[b, c, i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m], j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\n\u001b[1;32m    197\u001b[0m                 X[b, c, i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_size[\u001b[38;5;241m0\u001b[39m], j:j \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_size[\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[b, c, i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_size[\u001b[38;5;241m0\u001b[39m], j:j \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_size[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m    199\u001b[0m                 X[b, c, i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_size[\u001b[38;5;241m0\u001b[39m], j:j \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_size[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput[b, c, i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    200\u001b[0m                                                                            j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    201\u001b[0m                                                                            ]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/numpy/core/multiarray.py:341\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m \n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# both = yesset.concatenate(train_ds)\n",
    "model2.train(\n",
    "    src1_x,\n",
    "    src1_y,\n",
    "    epochs=50,\n",
    "    learning_rate=0.01,\n",
    "    batch_size=32,\n",
    "    optimizer='grad_descent',\n",
    ")\n",
    "\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=5)\n",
    "# history2 = model2.fit(train_ds, validation_data=test_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:28:46.962525Z",
     "start_time": "2024-06-09T13:28:46.835741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7d1114435240>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:28:52.835803Z",
     "start_time": "2024-06-09T13:28:46.963415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 - 4s - 13ms/step - accuracy: 0.9862 - loss: 0.0471\n",
      "65/65 - 1s - 12ms/step - accuracy: 0.9846 - loss: 0.0575\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "[3.0777485e-16 9.9999994e-01]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_acc = model.evaluate(yesset, verbose=2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "\n",
    "predictions = model.predict(test_ds)\n",
    "\n",
    "print(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:07:14.024674Z",
     "start_time": "2024-06-09T14:07:14.010554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3_0_0.jpg', 'notwaldo'),\n",
       " ('3_0_1.jpg', 'notwaldo'),\n",
       " ('3_0_10.jpg', 'notwaldo'),\n",
       " ('3_0_11.jpg', 'notwaldo'),\n",
       " ('3_0_12.jpg', 'notwaldo'),\n",
       " ('3_0_13.jpg', 'notwaldo'),\n",
       " ('3_0_14.jpg', 'notwaldo'),\n",
       " ('3_0_15.jpg', 'notwaldo'),\n",
       " ('3_0_2.jpg', 'notwaldo'),\n",
       " ('3_0_3.jpg', 'notwaldo'),\n",
       " ('3_0_4.jpg', 'notwaldo'),\n",
       " ('3_0_5.jpg', 'notwaldo'),\n",
       " ('3_0_6.jpg', 'notwaldo'),\n",
       " ('3_0_7.jpg', 'notwaldo'),\n",
       " ('3_0_8.jpg', 'notwaldo'),\n",
       " ('3_0_9.jpg', 'notwaldo'),\n",
       " ('3_10_0.jpg', 'notwaldo'),\n",
       " ('3_10_1.jpg', 'notwaldo'),\n",
       " ('3_10_10.jpg', 'notwaldo'),\n",
       " ('3_10_11.jpg', 'notwaldo'),\n",
       " ('3_10_12.jpg', 'notwaldo'),\n",
       " ('3_10_13.jpg', 'notwaldo'),\n",
       " ('3_10_14.jpg', 'notwaldo'),\n",
       " ('3_10_15.jpg', 'notwaldo'),\n",
       " ('3_10_2.jpg', 'notwaldo'),\n",
       " ('3_10_3.jpg', 'notwaldo'),\n",
       " ('3_10_4.jpg', 'notwaldo'),\n",
       " ('3_10_5.jpg', 'notwaldo'),\n",
       " ('3_10_6.jpg', 'notwaldo'),\n",
       " ('3_10_7.jpg', 'notwaldo'),\n",
       " ('3_10_8.jpg', 'notwaldo'),\n",
       " ('3_10_9.jpg', 'notwaldo'),\n",
       " ('3_11_0.jpg', 'notwaldo'),\n",
       " ('3_11_1.jpg', 'notwaldo'),\n",
       " ('3_11_10.jpg', 'notwaldo'),\n",
       " ('3_11_11.jpg', 'notwaldo'),\n",
       " ('3_11_12.jpg', 'notwaldo'),\n",
       " ('3_11_13.jpg', 'notwaldo'),\n",
       " ('3_11_14.jpg', 'notwaldo'),\n",
       " ('3_11_15.jpg', 'notwaldo'),\n",
       " ('3_11_2.jpg', 'notwaldo'),\n",
       " ('3_11_3.jpg', 'notwaldo'),\n",
       " ('3_11_4.jpg', 'notwaldo'),\n",
       " ('3_11_5.jpg', 'notwaldo'),\n",
       " ('3_11_6.jpg', 'notwaldo'),\n",
       " ('3_11_7.jpg', 'notwaldo'),\n",
       " ('3_11_8.jpg', 'notwaldo'),\n",
       " ('3_11_9.jpg', 'notwaldo'),\n",
       " ('3_12_0.jpg', 'notwaldo'),\n",
       " ('3_12_1.jpg', 'notwaldo'),\n",
       " ('3_12_10.jpg', 'notwaldo'),\n",
       " ('3_12_11.jpg', 'notwaldo'),\n",
       " ('3_12_12.jpg', 'notwaldo'),\n",
       " ('3_12_13.jpg', 'notwaldo'),\n",
       " ('3_12_14.jpg', 'notwaldo'),\n",
       " ('3_12_15.jpg', 'notwaldo'),\n",
       " ('3_12_2.jpg', 'notwaldo'),\n",
       " ('3_12_3.jpg', 'notwaldo'),\n",
       " ('3_12_4.jpg', 'notwaldo'),\n",
       " ('3_12_5.jpg', 'notwaldo'),\n",
       " ('3_12_6.jpg', 'notwaldo'),\n",
       " ('3_12_7.jpg', 'notwaldo'),\n",
       " ('3_12_8.jpg', 'notwaldo'),\n",
       " ('3_12_9.jpg', 'notwaldo'),\n",
       " ('3_13_0.jpg', 'notwaldo'),\n",
       " ('3_13_1.jpg', 'notwaldo'),\n",
       " ('3_13_10.jpg', 'notwaldo'),\n",
       " ('3_13_11.jpg', 'notwaldo'),\n",
       " ('3_13_12.jpg', 'notwaldo'),\n",
       " ('3_13_13.jpg', 'notwaldo'),\n",
       " ('3_13_14.jpg', 'notwaldo'),\n",
       " ('3_13_15.jpg', 'notwaldo'),\n",
       " ('3_13_2.jpg', 'notwaldo'),\n",
       " ('3_13_3.jpg', 'notwaldo'),\n",
       " ('3_13_4.jpg', 'notwaldo'),\n",
       " ('3_13_5.jpg', 'notwaldo'),\n",
       " ('3_13_6.jpg', 'notwaldo'),\n",
       " ('3_13_7.jpg', 'notwaldo'),\n",
       " ('3_13_8.jpg', 'notwaldo'),\n",
       " ('3_13_9.jpg', 'notwaldo'),\n",
       " ('3_14_0.jpg', 'notwaldo'),\n",
       " ('3_14_1.jpg', 'notwaldo'),\n",
       " ('3_14_10.jpg', 'notwaldo'),\n",
       " ('3_14_11.jpg', 'notwaldo'),\n",
       " ('3_14_12.jpg', 'notwaldo'),\n",
       " ('3_14_13.jpg', 'notwaldo'),\n",
       " ('3_14_14.jpg', 'notwaldo'),\n",
       " ('3_14_15.jpg', 'notwaldo'),\n",
       " ('3_14_2.jpg', 'notwaldo'),\n",
       " ('3_14_3.jpg', 'notwaldo'),\n",
       " ('3_14_4.jpg', 'notwaldo'),\n",
       " ('3_14_5.jpg', 'notwaldo'),\n",
       " ('3_14_6.jpg', 'notwaldo'),\n",
       " ('3_14_7.jpg', 'notwaldo'),\n",
       " ('3_14_8.jpg', 'notwaldo'),\n",
       " ('3_14_9.jpg', 'notwaldo'),\n",
       " ('3_15_0.jpg', 'notwaldo'),\n",
       " ('3_15_1.jpg', 'waldo'),\n",
       " ('3_15_10.jpg', 'notwaldo'),\n",
       " ('3_15_11.jpg', 'notwaldo'),\n",
       " ('3_15_12.jpg', 'notwaldo'),\n",
       " ('3_15_13.jpg', 'notwaldo'),\n",
       " ('3_15_14.jpg', 'notwaldo'),\n",
       " ('3_15_15.jpg', 'notwaldo'),\n",
       " ('3_15_2.jpg', 'notwaldo'),\n",
       " ('3_15_3.jpg', 'notwaldo'),\n",
       " ('3_15_4.jpg', 'notwaldo'),\n",
       " ('3_15_5.jpg', 'notwaldo'),\n",
       " ('3_15_6.jpg', 'notwaldo'),\n",
       " ('3_15_7.jpg', 'notwaldo'),\n",
       " ('3_15_8.jpg', 'notwaldo'),\n",
       " ('3_15_9.jpg', 'notwaldo'),\n",
       " ('3_1_0.jpg', 'notwaldo'),\n",
       " ('3_1_1.jpg', 'notwaldo'),\n",
       " ('3_1_10.jpg', 'notwaldo'),\n",
       " ('3_1_11.jpg', 'notwaldo'),\n",
       " ('3_1_12.jpg', 'notwaldo'),\n",
       " ('3_1_13.jpg', 'notwaldo'),\n",
       " ('3_1_14.jpg', 'notwaldo'),\n",
       " ('3_1_15.jpg', 'notwaldo'),\n",
       " ('3_1_2.jpg', 'notwaldo'),\n",
       " ('3_1_3.jpg', 'notwaldo'),\n",
       " ('3_1_4.jpg', 'notwaldo'),\n",
       " ('3_1_5.jpg', 'notwaldo'),\n",
       " ('3_1_6.jpg', 'notwaldo'),\n",
       " ('3_1_7.jpg', 'notwaldo'),\n",
       " ('3_1_8.jpg', 'notwaldo'),\n",
       " ('3_1_9.jpg', 'notwaldo'),\n",
       " ('3_2_0.jpg', 'notwaldo'),\n",
       " ('3_2_1.jpg', 'notwaldo'),\n",
       " ('3_2_10.jpg', 'notwaldo'),\n",
       " ('3_2_11.jpg', 'notwaldo'),\n",
       " ('3_2_12.jpg', 'notwaldo'),\n",
       " ('3_2_13.jpg', 'notwaldo'),\n",
       " ('3_2_14.jpg', 'notwaldo'),\n",
       " ('3_2_15.jpg', 'notwaldo'),\n",
       " ('3_2_2.jpg', 'notwaldo'),\n",
       " ('3_2_3.jpg', 'notwaldo'),\n",
       " ('3_2_4.jpg', 'notwaldo'),\n",
       " ('3_2_5.jpg', 'notwaldo'),\n",
       " ('3_2_6.jpg', 'notwaldo'),\n",
       " ('3_2_7.jpg', 'notwaldo'),\n",
       " ('3_2_8.jpg', 'notwaldo'),\n",
       " ('3_2_9.jpg', 'notwaldo'),\n",
       " ('3_3_0.jpg', 'notwaldo'),\n",
       " ('3_3_1.jpg', 'notwaldo'),\n",
       " ('3_3_10.jpg', 'notwaldo'),\n",
       " ('3_3_11.jpg', 'notwaldo'),\n",
       " ('3_3_12.jpg', 'notwaldo'),\n",
       " ('3_3_13.jpg', 'notwaldo'),\n",
       " ('3_3_14.jpg', 'notwaldo'),\n",
       " ('3_3_15.jpg', 'notwaldo'),\n",
       " ('3_3_2.jpg', 'notwaldo'),\n",
       " ('3_3_3.jpg', 'notwaldo'),\n",
       " ('3_3_4.jpg', 'notwaldo'),\n",
       " ('3_3_5.jpg', 'notwaldo'),\n",
       " ('3_3_6.jpg', 'notwaldo'),\n",
       " ('3_3_7.jpg', 'notwaldo'),\n",
       " ('3_3_8.jpg', 'notwaldo'),\n",
       " ('3_3_9.jpg', 'notwaldo'),\n",
       " ('3_4_0.jpg', 'notwaldo'),\n",
       " ('3_4_1.jpg', 'notwaldo'),\n",
       " ('3_4_10.jpg', 'notwaldo'),\n",
       " ('3_4_11.jpg', 'notwaldo'),\n",
       " ('3_4_12.jpg', 'notwaldo'),\n",
       " ('3_4_13.jpg', 'notwaldo'),\n",
       " ('3_4_14.jpg', 'notwaldo'),\n",
       " ('3_4_15.jpg', 'notwaldo'),\n",
       " ('3_4_2.jpg', 'notwaldo'),\n",
       " ('3_4_3.jpg', 'notwaldo'),\n",
       " ('3_4_4.jpg', 'notwaldo'),\n",
       " ('3_4_5.jpg', 'notwaldo'),\n",
       " ('3_4_6.jpg', 'notwaldo'),\n",
       " ('3_4_7.jpg', 'notwaldo'),\n",
       " ('3_4_8.jpg', 'notwaldo'),\n",
       " ('3_4_9.jpg', 'notwaldo'),\n",
       " ('3_5_0.jpg', 'notwaldo'),\n",
       " ('3_5_1.jpg', 'notwaldo'),\n",
       " ('3_5_10.jpg', 'notwaldo'),\n",
       " ('3_5_11.jpg', 'notwaldo'),\n",
       " ('3_5_12.jpg', 'notwaldo'),\n",
       " ('3_5_13.jpg', 'notwaldo'),\n",
       " ('3_5_14.jpg', 'notwaldo'),\n",
       " ('3_5_15.jpg', 'notwaldo'),\n",
       " ('3_5_2.jpg', 'notwaldo'),\n",
       " ('3_5_3.jpg', 'notwaldo'),\n",
       " ('3_5_4.jpg', 'notwaldo'),\n",
       " ('3_5_5.jpg', 'notwaldo'),\n",
       " ('3_5_6.jpg', 'notwaldo'),\n",
       " ('3_5_7.jpg', 'notwaldo'),\n",
       " ('3_5_8.jpg', 'notwaldo'),\n",
       " ('3_5_9.jpg', 'notwaldo'),\n",
       " ('3_6_0.jpg', 'notwaldo'),\n",
       " ('3_6_1.jpg', 'notwaldo'),\n",
       " ('3_6_10.jpg', 'notwaldo'),\n",
       " ('3_6_11.jpg', 'notwaldo'),\n",
       " ('3_6_12.jpg', 'notwaldo'),\n",
       " ('3_6_13.jpg', 'notwaldo'),\n",
       " ('3_6_14.jpg', 'notwaldo'),\n",
       " ('3_6_15.jpg', 'notwaldo'),\n",
       " ('3_6_2.jpg', 'notwaldo'),\n",
       " ('3_6_3.jpg', 'notwaldo'),\n",
       " ('3_6_4.jpg', 'notwaldo'),\n",
       " ('3_6_5.jpg', 'notwaldo'),\n",
       " ('3_6_6.jpg', 'notwaldo'),\n",
       " ('3_6_7.jpg', 'notwaldo'),\n",
       " ('3_6_8.jpg', 'notwaldo'),\n",
       " ('3_6_9.jpg', 'notwaldo'),\n",
       " ('3_7_0.jpg', 'notwaldo'),\n",
       " ('3_7_1.jpg', 'notwaldo'),\n",
       " ('3_7_10.jpg', 'notwaldo'),\n",
       " ('3_7_11.jpg', 'notwaldo'),\n",
       " ('3_7_12.jpg', 'notwaldo'),\n",
       " ('3_7_13.jpg', 'notwaldo'),\n",
       " ('3_7_14.jpg', 'notwaldo'),\n",
       " ('3_7_15.jpg', 'notwaldo'),\n",
       " ('3_7_2.jpg', 'notwaldo'),\n",
       " ('3_7_3.jpg', 'notwaldo'),\n",
       " ('3_7_4.jpg', 'notwaldo'),\n",
       " ('3_7_5.jpg', 'notwaldo'),\n",
       " ('3_7_6.jpg', 'notwaldo'),\n",
       " ('3_7_7.jpg', 'notwaldo'),\n",
       " ('3_7_8.jpg', 'notwaldo'),\n",
       " ('3_7_9.jpg', 'notwaldo'),\n",
       " ('3_8_0.jpg', 'notwaldo'),\n",
       " ('3_8_1.jpg', 'notwaldo'),\n",
       " ('3_8_10.jpg', 'notwaldo'),\n",
       " ('3_8_11.jpg', 'notwaldo'),\n",
       " ('3_8_12.jpg', 'notwaldo'),\n",
       " ('3_8_13.jpg', 'notwaldo'),\n",
       " ('3_8_14.jpg', 'notwaldo'),\n",
       " ('3_8_15.jpg', 'notwaldo'),\n",
       " ('3_8_2.jpg', 'notwaldo'),\n",
       " ('3_8_3.jpg', 'notwaldo'),\n",
       " ('3_8_4.jpg', 'notwaldo'),\n",
       " ('3_8_5.jpg', 'notwaldo'),\n",
       " ('3_8_6.jpg', 'notwaldo'),\n",
       " ('3_8_7.jpg', 'notwaldo'),\n",
       " ('3_8_8.jpg', 'notwaldo'),\n",
       " ('3_8_9.jpg', 'notwaldo'),\n",
       " ('3_9_0.jpg', 'notwaldo'),\n",
       " ('3_9_1.jpg', 'notwaldo'),\n",
       " ('3_9_10.jpg', 'notwaldo'),\n",
       " ('3_9_11.jpg', 'notwaldo'),\n",
       " ('3_9_12.jpg', 'notwaldo'),\n",
       " ('3_9_13.jpg', 'notwaldo'),\n",
       " ('3_9_14.jpg', 'notwaldo'),\n",
       " ('3_9_15.jpg', 'notwaldo'),\n",
       " ('3_9_2.jpg', 'notwaldo'),\n",
       " ('3_9_3.jpg', 'notwaldo'),\n",
       " ('3_9_4.jpg', 'notwaldo'),\n",
       " ('3_9_5.jpg', 'notwaldo'),\n",
       " ('3_9_6.jpg', 'notwaldo'),\n",
       " ('3_9_7.jpg', 'notwaldo'),\n",
       " ('3_9_8.jpg', 'notwaldo'),\n",
       " ('3_9_9.jpg', 'notwaldo')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "directory_path = f'../data/selfmade/'\n",
    "pref = \"3_\"\n",
    "filenames = [(f,'waldo') for f in os.listdir(directory_path+'/waldo') if f.startswith(pref)]+[(f, 'notwaldo') for f in os.listdir(directory_path+'/notwaldo') if f.startswith(pref)]\n",
    "filenames.sort()\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:07:26.252303Z",
     "start_time": "2024-06-09T14:07:15.991708Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:10<00:00, 24.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "correct=0\n",
    "incorrect=0\n",
    "incorrect_files=[]\n",
    "correct_files=[]\n",
    "# print(filenames)\n",
    "for fname, true in tqdm(filenames):\n",
    "    img = image.load_img(f'../data/src1/{IMG_SIZE}/{true}/{fname}', target_size=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Create a batch\n",
    "    # img_array /= 255.0  # Normalize the image to match the training data\n",
    "    \n",
    "    # Predict the class\n",
    "    prediction = model.predict(img_array, verbose =0)\n",
    "    predicted_class = 'waldo' if prediction[0][0] < prediction[0][1] else 'notwaldo'\n",
    "    # if true == 'notwaldo':\n",
    "        # print(true, prediction)\n",
    "    \n",
    "    # Print the prediction\n",
    "    if predicted_class==true:\n",
    "        correct += 1\n",
    "        correct_files+=[f'../data/src1/{IMG_SIZE}/{true}/{fname}']\n",
    "    else:\n",
    "        incorrect_files+=[f'../data/src1/{IMG_SIZE}/{true}/{fname}']\n",
    "        incorrect += 1\n",
    "    # print(f'Predi,cted class: {predicted_class} with {prediction[0]}\\t{\"CORRECT\" if predicted_class == True else \"INCORRECT\"}')\n",
    "    \n",
    "    # Optionally, display the image\n",
    "    # plt.imshow(img)\n",
    "    # plt.title(f'Predicted: {predicted_class}')\n",
    "    # plt.show()\n",
    "print(correct, incorrect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:07:26.893036Z",
     "start_time": "2024-06-09T14:07:26.889686Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/src1/64/notwaldo/3_0_14.jpg', '../data/src1/64/notwaldo/3_1_4.jpg', '../data/src1/64/notwaldo/3_2_1.jpg', '../data/src1/64/notwaldo/3_6_5.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(incorrect_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:05:59.753852Z",
     "start_time": "2024-06-09T14:05:59.748381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waldo correct:  1  incorrect:  1\n",
      "Notwaldo correct:  244  incorrect:  10\n"
     ]
    }
   ],
   "source": [
    "counter =0\n",
    "for f in correct_files:\n",
    "    if f.startswith(f'../data/src1/{IMG_SIZE}/waldo'):\n",
    "        counter+=1\n",
    "counter2 =0\n",
    "for f in incorrect_files:\n",
    "    if f.startswith(f'../data/src1/{IMG_SIZE}/waldo'):\n",
    "        counter2+=1 \n",
    "\n",
    "print(\"Waldo correct: \",counter, \" incorrect: \", counter2)\n",
    "\n",
    "counter =0\n",
    "for f in correct_files:\n",
    "    if f.startswith(f'../data/src1/{IMG_SIZE}/notwaldo'):\n",
    "        counter+=1\n",
    "counter2 =0\n",
    "for f in incorrect_files:\n",
    "    if f.startswith(f'../data/src1/{IMG_SIZE}/notwaldo'):\n",
    "        counter2+=1 \n",
    "\n",
    "print(\"Notwaldo correct: \",counter, \" incorrect: \", counter2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:29:05.712085Z",
     "start_time": "2024-06-09T13:29:02.363170Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 14/260\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 13ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 15:29:02.593169: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step\n",
      "0.4980122876761836\n"
     ]
    }
   ],
   "source": [
    "# ds = train_ds\n",
    "# true_labels = np.concatenate([y for x, y in ds], axis=0)\n",
    "# predictions = model.predict(ds)\n",
    "# predictions = np.argmax(predictions, axis=1)\n",
    "# counter=0\n",
    "# for true, pred in zip(true_labels, predictions):\n",
    "#     if true==pred:\n",
    "#         counter+=1\n",
    "#     # print(f\"True: {true}, Predicted: {pred}\")\n",
    "# print(counter/len(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:41:42.446998Z",
     "start_time": "2024-06-09T13:41:38.818046Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 15:41:40.043246: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n",
      "(126,) (array([  13,   21,  141,  212,  254,  268,  310,  316,  435,  441,  475,\n",
      "        614,  660,  724,  793,  809,  905,  940,  948,  994, 1026, 1030,\n",
      "       1047, 1118, 1119, 1146, 1173, 1248, 1267, 1306, 1370, 1412, 1437,\n",
      "       1548, 1562, 1591, 1606, 1611, 1665, 1750, 1791, 1864, 1883, 1969,\n",
      "       1988, 2010, 2059, 2071, 2095, 2126, 2164, 2194, 2294, 2369, 2375,\n",
      "       2406, 2445, 2638, 2640, 2708, 2730, 2741, 2757, 2801, 2869, 2876,\n",
      "       2922, 2981, 3075, 3137, 3145, 3155, 3161, 3182, 3183, 3184, 3202,\n",
      "       3282, 3305, 3307, 3319, 3352, 3402, 3452, 3578, 3584, 3622, 3653,\n",
      "       3734, 3754, 3758, 3833, 3865, 3907, 3916, 3997, 4022, 4132, 4164,\n",
      "       4184, 4188, 4198, 4221, 4395, 4396, 4398, 4419, 4562, 4581, 4584,\n",
      "       4613, 4706, 4731, 4830, 4888, 4921, 4970, 4999, 5003, 5037, 5039,\n",
      "       5125, 5148, 5154, 5251, 5314]),)\n",
      "(39,) (array([ 125,  284,  370,  391,  646,  794,  874,  971,  994, 1001, 1002,\n",
      "       1018, 1026, 1146, 1476, 1548, 1606, 1615, 1663, 2049, 2375, 2416,\n",
      "       2763, 2824, 3075, 3179, 3282, 3307, 3692, 4029, 4106, 4574, 4658,\n",
      "       4909, 5037, 5039, 5171, 5279, 5305]),)\n",
      "Manual checking accuracy on training data: 97.34%\n"
     ]
    }
   ],
   "source": [
    "# Unbatch and batch the dataset to get all data at once\n",
    "# train_images, train_labels = next(iter(src1_ds.unbatch().batch(len(train_ds))))\n",
    "train_images, train_labels = zip(*[(image.numpy(), label.numpy()) for image, label in src1_ds.unbatch()])\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "# test_images, test_labels = next(iter(yesset.unbatch().batch(len(test_ds))))\n",
    "\n",
    "# Predict on the training data\n",
    "train_predictions = model.predict(train_images)\n",
    "train_predicted_labels = tf.argmax(train_predictions, axis=1)\n",
    "print(np.where(train_predicted_labels == 1)[0].shape, np.where(train_predicted_labels == 1))\n",
    "print(np.where(train_labels == 1)[0].shape, np.where(train_labels == 1))\n",
    "# Ensure the labels are of the same type\n",
    "train_predicted_labels = tf.cast(train_predicted_labels, tf.int32)\n",
    "\n",
    "# Calculate the number of correct predictions\n",
    "correct_predictions = tf.reduce_sum(tf.cast(train_predicted_labels == train_labels, tf.int32))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / len(train_labels)\n",
    "\n",
    "print(f\"Manual checking accuracy on training data: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:42:53.388549Z",
     "start_time": "2024-06-09T13:42:53.296257Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('super_good_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
